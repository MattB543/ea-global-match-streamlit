#1. Qurat Zainab — Senior Program Associate, Open Philanthropy

    Why: She funds AI-risk capacity building and can directly tell you what kinds of “AI uplift” services/projects are most fundable, plus point you to Open Phil RFPs and decision-makers. This is one of the highest-leverage meetings for turning your pivot into a funded, credible plan.
    Topics to discuss: Your “AI Uplift” wedge + 2–3 scalable productized service concepts; what evidence/metrics Open Phil wants for tooling/ops/enablement; which org bottlenecks are most grantable in 2026.

#2. Alexandra Bos — Co-founder/CEO, Catalyze Impact

    Why: Catalyze incubates new AI safety/security orgs—exactly aligned with your founder skillset and your question of “join vs found.” She can help you evaluate whether to build a new org/product and connect you to cofounders/funders.
    Topics to discuss: Your potential venture theses (AI uplift as platform vs agency vs internal tooling); what Catalyze sees as the most “unowned” problems; whether you should apply to incubation and what milestones would derisk fundraising.

#3. Michael Aird — AI Program Director, Longview Philanthropy

    Why: He can give high-signal career/project advice and is actively hiring/funding across AI governance and entrepreneurship; he’s a strong reality-check for your roadmap. This meeting can convert your ideas into either a job pipeline or a fundable project plan.
    Topics to discuss: Your comparative advantage as founder-builder in AIS ecosystem; which governance/field-building projects most need product/engineering; what Longview looks for when funding founder-led initiatives.

#4. Ben Stewart — AI Program Officer, Longview Philanthropy

    Why: He explicitly wants to hear new project ideas and may fund “Better Futures” and institutional resilience work—great fit if your AI uplift expands into decision-support products. He can also sharpen your strategic worldview and connect you to relevant funders.
    Topics to discuss: A concrete “AI uplift” product path that improves org decision-making and reduces risk; where funding is bottlenecked vs talent; how to position your work as EV-positive and not just “nice-to-have ops.”

#5. Loic Watine — Director of EA Funds, Centre for Effective Altruism

    Why: He’s rebuilding EA Funds and hiring full-time grantmakers; meeting him can open hiring options and give you a high-level view of what funders want from scalable ecosystem infrastructure. Also useful for understanding how to align your “AI uplift” offering with funder expectations.
    Topics to discuss: What “AI grantmaking at EA Funds” will prioritize in 2026; how a services→product org can fit into EA funding models; whether there’s a role (or advisory relationship) that matches your background.

#6. Zachary Robinson — CEO, Centre for Effective Altruism

    Why: He’s hiring senior leadership (incl. ops/partnerships, AI grantmaking leadership) and shaping EA-wide priorities; he can give you a top-down map of ecosystem needs where your builder skillset is unusually valuable. A good meeting can generate warm intros and credible positioning for your pivot.
    Topics to discuss: Your pivot plan and where it best plugs into CEA/EV/EA Funds needs; what EA infrastructure is missing for AI-era productivity; how to design “AI uplift” to strengthen (not fragment) EA coordination.

#7. Benjamin Hilton — Head of Alignment & Head of LoC Mitigations, UK AI Security Institute

    Why: UK AISI is a major employer/funder hub and he’s hiring multiple roles spanning research + strategy/delivery; he can help you decide if you should target AISI roles or build external tooling for them. He also has a concrete view of what mitigations need implementation, not just research.
    Topics to discuss: Where AISI teams lose time today (tooling, workflows, eval pipelines) that you could productize; which open roles best match a founder/product engineer; collaboration pathways short of full-time employment.

#8. David Krueger — CEO, Evitable | Assistant Professor, Mila/University of Montreal

    Why: He’s building an outreach org and wants operational help and collaborators—this is a direct “founder + builder” opportunity where you could have large surface area impact quickly. He can also pressure-test your worldview and strategy around comms vs technical work.
    Topics to discuss: Whether “AI uplift” could be packaged as outreach/activation tooling (content ops, personalization, distribution analytics); what Evitable needs in its first 6–12 months; whether you should join as operator/cofounder-like contributor.

#9. Jacob Arbeid — Founder, AI Safety Org

    Why: He’s explicitly looking for a technical cofounder to build automated AI safety tooling (starting with evals)—this is one of the most direct matches to your SWE/product founder profile. Could convert into a serious cofounding path or a high-quality collaboration.
    Topics to discuss: Your fit for cofounding (equity, timeline, division of labor); product roadmap for eval automation agents; customer discovery plan across labs/AISIs/safety orgs and what to build first.

#10. Marius Hobbhahn — CEO, Apollo Research

    Why: Apollo is central in scheming/evals/monitoring and is expanding to the Bay; he’s open to talking about monitoring and for-profit ventures, and hiring. Meeting him can clarify where tooling/product work is most valuable and open job/collab options.
    Topics to discuss: Tooling gaps in monitoring/scheming workflows that a product builder can solve; whether Apollo needs “AI uplift” style enablement (internal tools, dashboards, pipelines); what a credible project proposal to Apollo would look like.

#11. Charlotte Stix — Head of AI Governance, Apollo Research

    Why: She sits at the intersection of frontier governance, national security implications, and internal deployment—excellent for shaping your product vision toward real-world constraints and policy relevance. She can also help you position your work to be legible to decision-makers.
    Topics to discuss: What governance teams actually need from technical tooling (audits, monitoring evidence, internal deployment controls); “AI uplift” offerings that would help governance output scale; gaps in current LoC governance practice.

#12. Jasmine Wang — Research Program Manager, Alignment, OpenAI

    Why: She leads a control team focused on mitigating code sabotage—deep overlap with operational safety + evaluation infrastructure where a builder can contribute. This meeting can uncover concrete problems that become consulting contracts, a role, or a product direction.
    Topics to discuss: What tooling/control environments are missing for sabotage mitigation; where program management meets engineering bottlenecks; what “exportable to other labs” control infrastructure would look like.

#13. Nicholas Joseph — Head of Pretraining, Anthropic

    Why: He has high-leverage insight into how frontier training teams operate and what kinds of safety/product infrastructure actually get adopted. He can advise on joining a lab vs building external tooling and potentially open recruiting paths.
    Topics to discuss: Where pretraining teams need better eval/monitoring/analytics tooling; what kinds of external tools vendors can realistically integrate; how to position yourself for a research engineering/product role at a frontier lab.

#14. Neel Nanda — Mechanistic Interpretability Team Lead, Google DeepMind

    Why: He has a rare combination of technical depth and pragmatic deployment thinking; he can help you understand what “interp in production” requires and what products could bridge research to deployment. Also useful for calibrating your own technical upskilling plan if you want more direct AIS research.
    Topics to discuss: Product opportunities around “pragmatic interpretability” and audits; what a credible alignment-audit toolchain needs; which research directions are ripe for tool-building vs still too early.

#15. Julian Stastny — Member of Technical Staff, Redwood Research

    Why: He leads low-stakes control at Redwood—directly relevant to building real control environments, tooling, and operationalization. Great source of concrete project ideas and potential collaboration paths.
    Topics to discuss: Where low-stakes control needs engineering leverage (envs, harnesses, monitoring UX); how to package your “AI uplift” to help control research teams; Redwood hiring/contracting pathways for builder-generalists.

#16. Tyler Tracy — Member of Technical Staff, Redwood Research

    Why: He’s working on high-stakes control environments and is thinking about exportability of research to labs—very aligned with your “scale value delivered” goal. He can give you a builder’s view of what infrastructure matters and where to contribute.
    Topics to discuss: “Bigger control environments” needs (tooling, data pipelines, eval harnesses); ideas for automatically finding scheming evidence; what would make control work more transferable to smaller safety teams.

#17. Megan Kinniment — Member of Technical Staff, METR

    Why: METR’s eval work is extremely infrastructure-heavy; she’s focused on scaling/automating evaluation and realistic tasks—prime territory for product engineering. This meeting can yield specific, high-impact tool ideas and possible hiring/collaboration.
    Topics to discuss: What automation or workflow tooling would most improve METR’s eval throughput and rigor; pain points in task creation and measurement; whether your “AI uplift” could standardize eval ops across orgs.

#18. Dmitrii Volkov — Head of Research, Palisade Research

    Why: Palisade has demonstrated ability to create high-visibility eval/security results; he leads a team of REs and is hiring senior talent. He can give you a hard-nosed perspective on what research+engineering outputs actually move policy and lab behavior.
    Topics to discuss: Tooling/infra needs for monitoring scheming and eliciting dangerous capabilities; how to design products that produce “headline-grade” evidence responsibly; whether Palisade needs an operator/product lead.

#19. Joseph Bloom — Head of White Box Evaluations, UK AI Security Institute

    Why: He’s driving concrete projects on monitorability/audit-resistance and has hiring/grant capacity—ideal if you want to build tooling that supports audits/monitoring. This can directly translate into scoped contractor work or a targeted role.
    Topics to discuss: Tooling to support sandbagging/auditing games workflows; what “evaluation awareness mitigations” need in practice; how to build a product that helps auditors generate credible evidence packages.

#20. Onni Aarne — Compute Team Technical Lead, IAPS

    Why: Compute governance is a major strategic lever; he’s deep on hardware-enabled verification mechanisms and can connect technical builders to projects/funding. This can broaden your pivot beyond “org productivity” into high-leverage governance tech.
    Topics to discuss: Productizable ideas around on-chip governance/flexHEGs; where software teams can contribute to hardware-enabled verification; what a credible pilot with cloud providers or policymakers would require.

#21. Daniel Eth — Senior Research Fellow & Director of Content, AI Policy Network (AIPN) | LTFF Fund Manager

    Why: He spans DC-facing policy work plus grantmaking; he can advise on which technical outputs best translate into policy wins and what gets funded. Useful if you want your tools/services to influence governance, not just internal org efficiency.
    Topics to discuss: How “AI uplift” could support congressional-facing work (briefings, evidence, demos); funding paths for policy-enabling tech; where AI R&D automation changes strategic priorities for builders.

#22. Ryan Kidd — Co-Executive Director, MATS Research

    Why: He’s building the talent + infrastructure pipeline and has Manifund funding—excellent for connecting your work to the broader ecosystem and for funding early prototypes. He can also help you find cofounders/hiring needs across programs.
    Topics to discuss: Where MATS/Astra/Constellation ecosystem needs shared tooling (matching, eval infra, research ops); whether a scalable “AI uplift” platform could be funded via Manifund; introductions to teams needing product/engineering help now.

#23. Isaac Dunn — Program Manager, Constellation

    Why: Constellation is a key hub for hosting and field-building; he thinks about UX and bottlenecks in AI safety fieldbuilding and is hiring. He can help you identify the highest-leverage “enablement” gaps and routes to embed in the ecosystem.
    Topics to discuss: Bottlenecks in AIS fieldbuilding that software/product could fix (matching, project discovery, collaboration tooling); what Constellation is missing in research program UX; whether there’s a role or contract path aligned with your skillset.

#24. Adeline Sinclair — Research Recruiting, Anthropic

    Why: She recruits across interpretability/RSP/education and has prior ops background; she can help you navigate Anthropic’s hiring process and calibrate which roles fit a founder/product engineer pivoting into AIS. Also useful for mapping “what labs actually hire for” in 2026.
    Topics to discuss: Best-fit roles at Anthropic for your profile (product/engineering/ops for safety); how to present “AI uplift” work credibly in lab hiring loops; which portfolio artifacts would most improve your candidacy.

#25. Abbey Chaver — Associate Program Officer, Coefficient Giving

    Why: She funds infosec/capacity building for AI and has hands-on ML product PM experience—highly aligned with your “services to scalable product” journey. She can point you to neglected infosec problems and potential funding pathways for tooling.
    Topics to discuss: Which AI infrastructure security problems are most neglected and toolable; how to scope a fundable MVP from your “AI uplift” offering; where productized security tooling could most reduce catastrophic risk.
